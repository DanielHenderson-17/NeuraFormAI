[
  {
    "objectID": "main_walkthrough.html",
    "href": "main_walkthrough.html",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "This walkthrough provides a detailed explanation of how main.py initializes and launches the NeuraPals application. Each section describes its purpose followed by the corresponding code.\n\n\n\nWe begin by importing all the required PyQt modules for creating the application window and layout. We also adjust the Python path to ensure imports from other modules work correctly.\n\nimport sys\nfrom pathlib import Path\nfrom PyQt6.QtWidgets import QApplication, QMainWindow, QWidget, QGridLayout, QSplashScreen\nfrom PyQt6.QtCore import Qt, QTimer\nfrom PyQt6.QtGui import QIcon, QPixmap\n\nfrom chat_ui.center.center_column_container import CenterColumnContainer\nfrom chat_ui.left.left_column_container import LeftColumnContainer\nfrom chat_ui.right.right_column_container import RightColumnContainer\n\n# ✅ Add project root to sys.path\nsys.path.append(str(Path(__file__).resolve().parent.parent))\n\n\n\n\n\nBefore the main window loads, we display a splash screen with the NeuraPals logo to indicate that the application is starting up.\n\napp = QApplication(sys.argv)\napp.setWindowIcon(QIcon(\"chat_ui/assets/neuraform_icon.png\"))\n\nsplash_pix = QPixmap(\"chat_ui/assets/neurapal_ai_splash.png\")\nsplash = QSplashScreen(splash_pix, Qt.WindowType.WindowStaysOnTopHint)\nsplash.setMask(splash_pix.mask())\nsplash.show()\napp.processEvents()\n\n\n\n\n\nHere, we create the main application window and initialize a grid layout to hold different parts of the UI.\n\nwindow = QMainWindow()\nwindow.setWindowTitle(\"NeuraPals - AI Chat\")\nwindow.resize(1400, 700)\n\nmain_widget = QWidget()\nlayout = QGridLayout(main_widget)\nlayout.setContentsMargins(0, 0, 0, 0)\nlayout.setSpacing(0)\n\n\n\n\n\nWe add three main UI sections: left column (e.g., navigation or tools), center column (chat input), and right column (chat messages).\n\n# === Left column\nleft_column = LeftColumnContainer()\nlayout.addWidget(left_column, 0, 0, 2, 1)\n\n# === Center column (includes chat_input internally)\ncenter_column = CenterColumnContainer()\nlayout.addWidget(center_column, 0, 1, 2, 1)\n\n# === Right column\nright_column = RightColumnContainer()\nlayout.addWidget(right_column, 0, 2, 2, 1)\n\n\n\n\n\nWe connect the chat input component from the center column with the chat window on the right column to enable message flow.\n\ncenter_column.chat_input.chat_window = right_column.chat_window\nright_column.chat_window.input_box = center_column.chat_input\n\n\n\n\n\nTo make the layout responsive, we set the column stretching to give more space to the center column and equal but smaller space to the left and right columns.\n\nlayout.setColumnStretch(0, 1)\nlayout.setColumnStretch(1, 2)\nlayout.setColumnStretch(2, 1)\n\n\n\n\n\nWe finish setting up the layout, position the window at the screen’s center, close the splash screen after a short delay, and then display the main application window.\n\nwindow.setCentralWidget(main_widget)\nwindow.move(app.primaryScreen().availableGeometry().center() - window.rect().center())\n\nQTimer.singleShot(1500, splash.close)  # Keeps splash for 1.5s\nQTimer.singleShot(1500, window.show)\n\nsys.exit(app.exec())\n\n\n\n\n\nFinally, we ensure that main() is executed only if this script is run directly (not imported as a module).\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "docs.html",
    "href": "docs.html",
    "title": "docs",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "docs.html#quarto",
    "href": "docs.html#quarto",
    "title": "docs",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to NeuraPals Docs",
    "section": "",
    "text": "Welcome to NeuraPals Docs"
  },
  {
    "objectID": "main_walkthrough.html#imports-and-project-path-setup",
    "href": "main_walkthrough.html#imports-and-project-path-setup",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "We begin by importing all the required PyQt modules for creating the application window and layout. We also adjust the Python path to ensure imports from other modules work correctly.\n\nimport sys\nfrom pathlib import Path\nfrom PyQt6.QtWidgets import QApplication, QMainWindow, QWidget, QGridLayout, QSplashScreen\nfrom PyQt6.QtCore import Qt, QTimer\nfrom PyQt6.QtGui import QIcon, QPixmap\n\nfrom chat_ui.center.center_column_container import CenterColumnContainer\nfrom chat_ui.left.left_column_container import LeftColumnContainer\nfrom chat_ui.right.right_column_container import RightColumnContainer\n\n# ✅ Add project root to sys.path\nsys.path.append(str(Path(__file__).resolve().parent.parent))"
  },
  {
    "objectID": "main_walkthrough.html#show-splash-screen",
    "href": "main_walkthrough.html#show-splash-screen",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "Before the main window loads, we display a splash screen with the NeuraPals logo to indicate that the application is starting up.\n\napp = QApplication(sys.argv)\napp.setWindowIcon(QIcon(\"chat_ui/assets/neuraform_icon.png\"))\n\nsplash_pix = QPixmap(\"chat_ui/assets/neurapal_ai_splash.png\")\nsplash = QSplashScreen(splash_pix, Qt.WindowType.WindowStaysOnTopHint)\nsplash.setMask(splash_pix.mask())\nsplash.show()\napp.processEvents()"
  },
  {
    "objectID": "main_walkthrough.html#setup-main-window",
    "href": "main_walkthrough.html#setup-main-window",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "Here, we create the main application window and initialize a grid layout to hold different parts of the UI.\n\nwindow = QMainWindow()\nwindow.setWindowTitle(\"NeuraPals - AI Chat\")\nwindow.resize(1400, 700)\n\nmain_widget = QWidget()\nlayout = QGridLayout(main_widget)\nlayout.setContentsMargins(0, 0, 0, 0)\nlayout.setSpacing(0)"
  },
  {
    "objectID": "main_walkthrough.html#add-ui-columns",
    "href": "main_walkthrough.html#add-ui-columns",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "We add three main UI sections: left column (e.g., navigation or tools), center column (chat input), and right column (chat messages).\n\n# === Left column\nleft_column = LeftColumnContainer()\nlayout.addWidget(left_column, 0, 0, 2, 1)\n\n# === Center column (includes chat_input internally)\ncenter_column = CenterColumnContainer()\nlayout.addWidget(center_column, 0, 1, 2, 1)\n\n# === Right column\nright_column = RightColumnContainer()\nlayout.addWidget(right_column, 0, 2, 2, 1)"
  },
  {
    "objectID": "main_walkthrough.html#wire-up-chat-input",
    "href": "main_walkthrough.html#wire-up-chat-input",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "We connect the chat input component from the center column with the chat window on the right column to enable message flow.\n\ncenter_column.chat_input.chat_window = right_column.chat_window\nright_column.chat_window.input_box = center_column.chat_input"
  },
  {
    "objectID": "main_walkthrough.html#column-layout-proportions",
    "href": "main_walkthrough.html#column-layout-proportions",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "To make the layout responsive, we set the column stretching to give more space to the center column and equal but smaller space to the left and right columns.\n\nlayout.setColumnStretch(0, 1)\nlayout.setColumnStretch(1, 2)\nlayout.setColumnStretch(2, 1)"
  },
  {
    "objectID": "main_walkthrough.html#final-window-setup-and-launch",
    "href": "main_walkthrough.html#final-window-setup-and-launch",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "We finish setting up the layout, position the window at the screen’s center, close the splash screen after a short delay, and then display the main application window.\n\nwindow.setCentralWidget(main_widget)\nwindow.move(app.primaryScreen().availableGeometry().center() - window.rect().center())\n\nQTimer.singleShot(1500, splash.close)  # Keeps splash for 1.5s\nQTimer.singleShot(1500, window.show)\n\nsys.exit(app.exec())"
  },
  {
    "objectID": "main_walkthrough.html#entry-point",
    "href": "main_walkthrough.html#entry-point",
    "title": "Main.py Walkthrough",
    "section": "",
    "text": "Finally, we ensure that main() is executed only if this script is run directly (not imported as a module).\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "voice_recorder_walkthrough.html",
    "href": "voice_recorder_walkthrough.html",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "This walkthrough explains how the VoiceRecorder class captures microphone input, detects speech, and transcribes spoken words into text using Faster Whisper. Each section includes descriptive notes and the corresponding code snippet.\n\n\n\nWe begin by importing all the necessary modules. sounddevice handles microphone input, numpy processes audio data, webrtcvad detects speech activity, and faster_whisper provides fast speech-to-text transcription.\n\nimport sounddevice as sd\nimport numpy as np\nimport queue\nimport threading\nimport webrtcvad\nfrom faster_whisper import WhisperModel\nimport time\n\n\n\n\n\nThe VoiceRecorder class initializes with audio configuration settings, a speech detection model, and state variables to manage the recording process.\n\nclass VoiceRecorder:\n    def __init__(self, model_name=\"base\", silence_duration=4.0, aggressiveness=3):\n        self.sample_rate = 16000\n        self.block_duration = 30\n        self.block_size = int(self.sample_rate * self.block_duration / 1000)\n        self.vad = webrtcvad.Vad(aggressiveness)\n        self.audio_queue = queue.Queue()\n        self.recording = False\n        self.model = WhisperModel(model_name, compute_type=\"int8\", device=\"cpu\")\n        self.silence_duration = silence_duration\n        self._last_voice_time = time.time()\n        self.continuous_mode = True\n        self.status_callback = None\n        self.max_empty_loops = 3\n\n\n\n\n\nThe _callback method is triggered every time an audio block is captured from the microphone. It queues the audio data for further processing.\n\ndef _callback(self, indata, frames, time_info, status):\n    if status:\n        print(\"Status:\", status)\n    audio_data = indata[:, 0]\n    self.audio_queue.put(audio_data.copy())\n\n\n\n\n\nThis method converts the audio chunk into PCM format and checks if it contains speech using WebRTC’s Voice Activity Detection (VAD).\n\ndef _is_speech(self, chunk):\n    pcm = (chunk * 32768).astype(np.int16).tobytes()\n    return self.vad.is_speech(pcm, self.sample_rate)\n\n\n\n\n\nThe _record_until_silence method continuously captures audio until a period of silence is detected, helping segment speech for transcription.\n\ndef _record_until_silence(self):\n    if self.status_callback:\n        self.status_callback(\"Listening...\")\n    audio = []\n    with sd.InputStream(\n        channels=1,\n        samplerate=self.sample_rate,\n        blocksize=self.block_size,\n        dtype=\"float32\",\n        callback=self._callback\n    ):\n        self._last_voice_time = time.time()\n        while self.recording:\n            try:\n                block = self.audio_queue.get(timeout=1)\n            except queue.Empty:\n                continue\n            audio.append(block)\n            if self._is_speech(block):\n                self._last_voice_time = time.time()\n            elif time.time() - self._last_voice_time &gt; self.silence_duration:\n                print(f\"⏹️ Silence detected — stopping chunk after {time.time() - self._last_voice_time:.2f}s\")\n                break\n    return np.concatenate(audio)\n\n\n\n\n\nOnce audio is recorded, this method uses Faster Whisper to transcribe speech into text and returns the final transcript.\n\ndef _transcribe(self, audio):\n    if self.status_callback:\n        self.status_callback(\"Transcribing...\")\n    print(\"🧠 Transcribing...\")\n    segments, _ = self.model.transcribe(audio, language=\"en\")\n    full_text = \" \".join(segment.text for segment in segments)\n    print(\"📝 Transcript:\", full_text)\n    return full_text\n\n\n\n\n\nThe main loop continuously records audio, transcribes it, and handles silent segments. If silence persists for multiple attempts, it auto-stops the recording.\n\ndef _run_loop(self, callback):\n    empty_count = 0\n    while self.recording:\n        audio = self._record_until_silence()\n        if not self.recording:\n            break\n        transcript = self._transcribe(audio).strip()\n        if transcript:\n            empty_count = 0\n            callback(transcript)\n        else:\n            empty_count += 1\n            print(f\"⚠️ Skipped empty transcript. ({empty_count}/{self.max_empty_loops})\")\n            if self.status_callback:\n                self.status_callback(f\"Silent ({empty_count}/{self.max_empty_loops})\")\n        if empty_count &gt;= self.max_empty_loops:\n            print(\"🛑 Auto-stopping after too many silent loops.\")\n            if self.status_callback:\n                self.status_callback(\"Stopped.\")\n            self.stop()\n            return\n        if not self.continuous_mode:\n            break\n    self.recording = False\n    if self.status_callback:\n        self.status_callback(\"Stopped.\")\n    print(\"🛑 VoiceRecorder stopped\")\n\n\n\n\n\nThis method starts the voice recording process in a background thread to avoid blocking the main application.\n\ndef start_recording_async(self, callback, on_status=None):\n    if self.recording:\n        print(\"⚠️ Already recording. Ignored.\")\n        return\n    self.recording = True\n    self.status_callback = on_status\n    thread = threading.Thread(target=self._run_loop, args=(callback,), daemon=True)\n    thread.start()\n\n\n\n\n\nStops the voice recorder, ensuring that no additional audio is captured or transcribed.\n\ndef stop(self):\n    print(\"⏹️ Manual stop called\")\n    self.recording = False"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#imports-and-setup",
    "href": "voice_recorder_walkthrough.html#imports-and-setup",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "We begin by importing all the necessary modules. sounddevice handles microphone input, numpy processes audio data, webrtcvad detects speech activity, and faster_whisper provides fast speech-to-text transcription.\n\nimport sounddevice as sd\nimport numpy as np\nimport queue\nimport threading\nimport webrtcvad\nfrom faster_whisper import WhisperModel\nimport time"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#class-initialization",
    "href": "voice_recorder_walkthrough.html#class-initialization",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "The VoiceRecorder class initializes with audio configuration settings, a speech detection model, and state variables to manage the recording process.\n\nclass VoiceRecorder:\n    def __init__(self, model_name=\"base\", silence_duration=4.0, aggressiveness=3):\n        self.sample_rate = 16000\n        self.block_duration = 30\n        self.block_size = int(self.sample_rate * self.block_duration / 1000)\n        self.vad = webrtcvad.Vad(aggressiveness)\n        self.audio_queue = queue.Queue()\n        self.recording = False\n        self.model = WhisperModel(model_name, compute_type=\"int8\", device=\"cpu\")\n        self.silence_duration = silence_duration\n        self._last_voice_time = time.time()\n        self.continuous_mode = True\n        self.status_callback = None\n        self.max_empty_loops = 3"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#audio-callback",
    "href": "voice_recorder_walkthrough.html#audio-callback",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "The _callback method is triggered every time an audio block is captured from the microphone. It queues the audio data for further processing.\n\ndef _callback(self, indata, frames, time_info, status):\n    if status:\n        print(\"Status:\", status)\n    audio_data = indata[:, 0]\n    self.audio_queue.put(audio_data.copy())"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#speech-detection",
    "href": "voice_recorder_walkthrough.html#speech-detection",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "This method converts the audio chunk into PCM format and checks if it contains speech using WebRTC’s Voice Activity Detection (VAD).\n\ndef _is_speech(self, chunk):\n    pcm = (chunk * 32768).astype(np.int16).tobytes()\n    return self.vad.is_speech(pcm, self.sample_rate)"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#recording-loop",
    "href": "voice_recorder_walkthrough.html#recording-loop",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "The _record_until_silence method continuously captures audio until a period of silence is detected, helping segment speech for transcription.\n\ndef _record_until_silence(self):\n    if self.status_callback:\n        self.status_callback(\"Listening...\")\n    audio = []\n    with sd.InputStream(\n        channels=1,\n        samplerate=self.sample_rate,\n        blocksize=self.block_size,\n        dtype=\"float32\",\n        callback=self._callback\n    ):\n        self._last_voice_time = time.time()\n        while self.recording:\n            try:\n                block = self.audio_queue.get(timeout=1)\n            except queue.Empty:\n                continue\n            audio.append(block)\n            if self._is_speech(block):\n                self._last_voice_time = time.time()\n            elif time.time() - self._last_voice_time &gt; self.silence_duration:\n                print(f\"⏹️ Silence detected — stopping chunk after {time.time() - self._last_voice_time:.2f}s\")\n                break\n    return np.concatenate(audio)"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#transcription",
    "href": "voice_recorder_walkthrough.html#transcription",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "Once audio is recorded, this method uses Faster Whisper to transcribe speech into text and returns the final transcript.\n\ndef _transcribe(self, audio):\n    if self.status_callback:\n        self.status_callback(\"Transcribing...\")\n    print(\"🧠 Transcribing...\")\n    segments, _ = self.model.transcribe(audio, language=\"en\")\n    full_text = \" \".join(segment.text for segment in segments)\n    print(\"📝 Transcript:\", full_text)\n    return full_text"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#main-recording-loop",
    "href": "voice_recorder_walkthrough.html#main-recording-loop",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "The main loop continuously records audio, transcribes it, and handles silent segments. If silence persists for multiple attempts, it auto-stops the recording.\n\ndef _run_loop(self, callback):\n    empty_count = 0\n    while self.recording:\n        audio = self._record_until_silence()\n        if not self.recording:\n            break\n        transcript = self._transcribe(audio).strip()\n        if transcript:\n            empty_count = 0\n            callback(transcript)\n        else:\n            empty_count += 1\n            print(f\"⚠️ Skipped empty transcript. ({empty_count}/{self.max_empty_loops})\")\n            if self.status_callback:\n                self.status_callback(f\"Silent ({empty_count}/{self.max_empty_loops})\")\n        if empty_count &gt;= self.max_empty_loops:\n            print(\"🛑 Auto-stopping after too many silent loops.\")\n            if self.status_callback:\n                self.status_callback(\"Stopped.\")\n            self.stop()\n            return\n        if not self.continuous_mode:\n            break\n    self.recording = False\n    if self.status_callback:\n        self.status_callback(\"Stopped.\")\n    print(\"🛑 VoiceRecorder stopped\")"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#start-recording",
    "href": "voice_recorder_walkthrough.html#start-recording",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "This method starts the voice recording process in a background thread to avoid blocking the main application.\n\ndef start_recording_async(self, callback, on_status=None):\n    if self.recording:\n        print(\"⚠️ Already recording. Ignored.\")\n        return\n    self.recording = True\n    self.status_callback = on_status\n    thread = threading.Thread(target=self._run_loop, args=(callback,), daemon=True)\n    thread.start()"
  },
  {
    "objectID": "voice_recorder_walkthrough.html#stop-recording",
    "href": "voice_recorder_walkthrough.html#stop-recording",
    "title": "Voice Recorder Walkthrough",
    "section": "",
    "text": "Stops the voice recorder, ensuring that no additional audio is captured or transcribed.\n\ndef stop(self):\n    print(\"⏹️ Manual stop called\")\n    self.recording = False"
  }
]